# __Project Name:__ Implementing a Data Warehouse on AWS to Sparkify Startup

__Data Engineer:__ Lysmar Q Freitas

## 1. Introduction

Sparkify, a music streaming startup, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I was tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.


## 2. Project Description
Move Sparkify's processes and data onto the cloud with AWS, building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

## 3. Project Datasets

The project will work with two datasets that reside in S3 on Udacity Nanodegree workspace. Here are the S3 links for each:

- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

Log data json path: s3://udacity-dend/log_json_path.json

### 3.1. Song Dataset1
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

- song_data/A/B/C/TRABCEI128F424C983.json
- song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### 3.2. Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

- log_data/2018/11/2018-11-12-events.json
- log_data/2018/11/2018-11-13-events.json

And below is an example of what a single log file, 2018-11-13-events.json, looks like.

{"artist":"Pavement", "auth":"Logged In", "firstName":"Sylvie", "gender", "F", "itemInSession":0, "lastName":"Cruz", "length":99.16036, "level":"free", "location":"Klamath Falls, OR", "method":"PUT", "page":"NextSong", "registration":"1.541078e+12", "sessionId":345, "song":"Mercy:The Laundromat", "status":200, "ts":1541990258796, "userAgent":"Mozilla/5.0(Macintosh; Intel Mac OS X 10_9_4...)", "userId":10}



## 4. Project Development
Below are the steps followed to complete this project.

### 4.1 Create Table Schemas

#### 4.1.1 Design schemas for Song Play Analysis
Using the song and event datasets, it was created a star schema optimized for queries on song play analysis. This includes the following tables.

##### Fact Table
__songplays -__ records in event data associated with song plays i.e. records with page NextSong
- songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

##### Dimension Tables
__users -__ users in the app
- user_id, first_name, last_name, gender, level

__songs -__ songs in music database
- song_id, title, artist_id, year, duration

__artists -__ artists in music database
- artist_id, name, location, lattitude, longitude

__time -__ timestamps of records in songplays broken down into specific units
- start_time, hour, day, week, month, year, weekday

#### 4.1.2 Write SQL statements for each of these tables in script _sql_queries.py_
In order to create and load the tables properly, the following groups of SQL queries were used:

- #DROP TABLE QUERIES  
  DROP TABLE IF EXISTS __table_name__

- #CREATE TABLE QUERIES  
  CREATE TABLE IF NOT EXISTS __table_name__

- #COPY TABLE QUERIES / STAGING TABLES  
  COPY table_name FROM __parameters__

- #INSERT TABLE QUERIES / ANALYTICS TABLES  
  INSERT INTO __table_name__

#### 4.1.3. Complete the logic in script _create_tables.py_ to connect to the database, drop and create tables
Three functions were defined. One to connect to the Sparkfy database and the others to drop and create tables, using the SQL queries DROP TABLES and CREATE TABLES from last step.

#### 4.1.4. Launch a redshift cluster and create an IAM role that has read access to S3.
An IAM role with access to S3 was created in order to procede with the launch of the redshift cluster on region "us-west-2".

#### 4.1.5. Add redshift database and IAM role info to dwh.cfg.
The paremeters of cluster, IAM role and S3 were defined in dwh.cfg in order to make the proper connections.



### 4.2. Build ETL Pipeline
#### 4.2.1. Implement the logic in the script _etl.py_ to load data from S3 to staging tables on Redshift
One function was defined to load data from S3 connect to staging tables, using the SQL queries STAGING TABLES.


#### 4.2.2. Implement the logic in the script _etl.py_ to load data from staging tables to analytics tables on Redshift.
One function was defined to load data from staging tables to analytics tables, using the SQL queries ANALYTICS TABLES.


### 4.3. Run and test the scripts _create_tables.py_ and _etl.py_
- Using the terminal, the script _create_tables.py_ was run and successfully connected to the Sparkify database, dropped any tables if they existed, and created the tables.

 - __On the terminal, run the command:__ python3 create_tables.py__

- The same with the script _etl.py_. that connected to the Sparkify redshift database, loaded log_data and song_data into staging tables, and transformed them into the five analytics tables.

 - __On the terminal, run the command:__ python3 etl.py__


## 5. Conclusion
The project successfully moved the Sparkify's datasets to a Data Warehouse on the AWS, building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of fact and dimensional tables. This will enable Sparkify analytics team to find more detailed insights in what songs their users are listening to.
